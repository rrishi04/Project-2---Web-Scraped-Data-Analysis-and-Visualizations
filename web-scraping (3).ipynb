{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bec21d",
   "metadata": {},
   "source": [
    "# Web Scraping with requests and Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4f506",
   "metadata": {},
   "source": [
    "#### Imports Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98cb8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb3ce2",
   "metadata": {},
   "source": [
    "#### Get URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903cf440",
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.basketball-reference.com/players/s/simshe01.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190173cd",
   "metadata": {},
   "source": [
    "#### Headers to not overload systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51d3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"http://example.com/previous-page\",\n",
    "    \"Cookie\": \"session_id=your_session_id_here\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6a364",
   "metadata": {},
   "source": [
    "#### Get responses from website and return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645e733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb843590",
   "metadata": {},
   "source": [
    "#### Return weather you were able to scrap or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b3cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! Received status code 429\n"
     ]
    }
   ],
   "source": [
    "if status == 200:\n",
    "    page = response.text\n",
    "    soup = bs(page)\n",
    "    print(\"HTML Recieved!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Oops! Received status code {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77da7d8",
   "metadata": {},
   "source": [
    "#### Print HTML if obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8abcfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61ea18",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "\n",
    "#### Create function for getting HTML Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda14b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_page(url):\n",
    "    # Headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"http://example.com/previous-page\",\n",
    "        \"Cookie\": \"session_id=your_session_id_here\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cache-Control\": \"max-age=0\"\n",
    "    }\n",
    "    # Response and status saved\n",
    "    response = requests.get(url, headers=headers)\n",
    "    status = response.status_code\n",
    "    \n",
    "    # Return if HTML obtained\n",
    "    if status == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Oops! Received status code {status}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ec26b",
   "metadata": {},
   "source": [
    "#### Get URLS and pipline list ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd722e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URl\n",
    "base = \"https://www.basketball-reference.com\"\n",
    "url_start = f\"{base}/players/\"\n",
    "\n",
    "# List of Info\n",
    "pipeline_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb32644",
   "metadata": {},
   "source": [
    "#### Get HTML and soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = get_web_page(url_start)\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cc03e",
   "metadata": {},
   "source": [
    "#### Find hidden tags within HTML Comments and only keeps URL linked comments *--Used Help from ChatGPT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a709ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all comments in HTML\n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "# Empty List\n",
    "letter_links = []\n",
    "\n",
    "# Loop through comments to only keep the ones with URLS and remove the rest\n",
    "for c in comments:\n",
    "    if '/players/' in c:\n",
    "        comment_soup = bs(c, \"html.parser\")\n",
    "        for a in comment_soup.select(\"a[href^='/players/']\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and len(href) == 11:\n",
    "                letter_links.append(base + href)\n",
    "\n",
    "# Print links found\n",
    "print(f\"Found {len(letter_links)} letter pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bcf34",
   "metadata": {},
   "source": [
    "#### Gather all Player Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Links\n",
    "player_links = []\n",
    "\n",
    "# Loop through the URLs and obtain HTML returns of those pages\n",
    "for letter_url in letter_links:\n",
    "    page = get_web_page(letter_url)\n",
    "    soup = bs(page, \"html.parser\")\n",
    "\n",
    "    # From HTML get the extact player pages\n",
    "    for a in soup.select(\"th[data-stat='player'] a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            player_links.append(base + href)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print out total player links\n",
    "print(f\"Collected {len(player_links)} player profile links.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806cbc6",
   "metadata": {},
   "source": [
    "#### Loop through 5 players and scrape each part of their stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270eeb6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'player_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loop through first 5 players\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mplayer_links\u001b[49m[:\u001b[38;5;241m5\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Get the HTML and soup of the player\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     html \u001b[38;5;241m=\u001b[39m get_web_page(url)\n\u001b[0;32m      6\u001b[0m     soup \u001b[38;5;241m=\u001b[39m bs(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'player_links' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop through first 5 players\n",
    "for i, url in enumerate(player_links[:5]):\n",
    "    \n",
    "    # Get the HTML and soup of the player\n",
    "    html = get_web_page(url)\n",
    "    soup = bs(html, \"html.parser\")\n",
    "\n",
    "    # Get name\n",
    "    name = soup.find(\"h1\").find(\"span\").text.strip() if soup.find(\"h1\") else \"N/A\"\n",
    "\n",
    "    # Get p tags\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "\n",
    "    # Converts each paragraph into text\n",
    "    for p in p_tags:\n",
    "        text = p.get_text(\" \", strip=True)\n",
    "    \n",
    "        # Look for the Position\n",
    "        if \"Position:\" in text:\n",
    "            # Extract the position value only *got help from chatGPT\n",
    "            position = (\n",
    "                text.split(\"Position:\")[1]\n",
    "                .split(\"Shoots:\")[0]\n",
    "                .split(\"â–ª\")[0]\n",
    "                .strip()\n",
    "            )\n",
    "    \n",
    "        # Look for the Weight\n",
    "        if \"lb\" in text:\n",
    "            spans = p.find_all(\"span\")\n",
    "            if len(spans) > 1:\n",
    "                # Extract the weight value only\n",
    "                weight_text = spans[1].text.strip()\n",
    "                if \"lb\" in weight_text:\n",
    "                    weight = int(weight_text.replace(\"lb\", \"\").strip())\n",
    "\n",
    "\n",
    "        # mp_per_g tag\n",
    "        minute_tags = soup.find_all(\"td\", {\"data-stat\": \"mp_per_g\"})\n",
    "        # extract and convert to float\n",
    "        minutes_played = [float(tag.text.strip()) for tag in minute_tags if tag.text.strip()]\n",
    "\n",
    "        # pts_per_g tag\n",
    "        ppg_tags = soup.find_all(\"td\", {\"data-stat\": \"pts_per_g\"})\n",
    "        # extract and convert to float\n",
    "        points_per_game = [float(tag.text.strip()) for tag in ppg_tags if tag.text.strip()]\n",
    "          \n",
    "        # fta_per_g tag\n",
    "        fta_tags = soup.find_all(\"td\", {\"data-stat\": \"fta_per_g\"})\n",
    "        # Create empty list\n",
    "        free_throws = []\n",
    "        # Loop throuhg the tag and add values to list * got help from chatGPT\n",
    "        for tag in fta_tags:\n",
    "            text = tag.text.strip()\n",
    "            if text:  # skip empty cells\n",
    "                try:\n",
    "                    free_throws.append(float(text))\n",
    "                except ValueError:\n",
    "                    pass  # ignore non-numeric text\n",
    "        # if no values resort to 0\n",
    "        if not free_throws:\n",
    "            free_throws = [0]\n",
    "\n",
    "        # blk_per_g tag\n",
    "        blk_tags = soup.find_all(\"td\", {\"data-stat\": \"blk_per_g\"})\n",
    "        # extract and convert to float *got help from chatGPT\n",
    "        blocks = float(blk_tags[-1].text.strip()) if blk_tags and blk_tags[-1].text.strip() else 0\n",
    "        \n",
    "        # games total\n",
    "        games_label = soup.find(\"span\", {\"class\": \"poptip\", \"data-tip\": \"Games\"})\n",
    "        if games_label:\n",
    "            # find the p tag and get the total game\n",
    "            p_tag = games_label.find_next(\"p\")\n",
    "            if p_tag and p_tag.text.strip().isdigit():\n",
    "                total_games = int(p_tag.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Create a big dictionary for each player with their corresponding stats\n",
    "        player_dict = {\n",
    "        \"name\": name,\n",
    "        \"position\": position,\n",
    "        \"weight\": weight,\n",
    "        \"minutes played\": minutes_played,\n",
    "        \"points\": points_per_game,\n",
    "        \"free_throws\": free_throws,\n",
    "        \"blocks\": blocks,\n",
    "        \"total_games\": total_games\n",
    "        }\n",
    "        \n",
    "        # Add to pipeline list\n",
    "        pipeline_list.append(player_dict)\n",
    "    \n",
    "    # sleep between requests\n",
    "    time.sleep(2)\n",
    "\n",
    "# Review collected data\n",
    "print(f\"\\nTotal players scraped: {len(pipeline_list)}\")\n",
    "pipeline_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e8d9a",
   "metadata": {},
   "source": [
    "#### Start to move data to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fe274-e64b-49f2-b392-0d60ba6cc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of player dictionaries to DataFrame\n",
    "df = pd.DataFrame(pipeline_list)\n",
    "\n",
    "# Standardize column names to avoid spaces/case issues\n",
    "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "# --- Compute averages for list-based or single-value stats ---\n",
    "for col, new_col in [\n",
    "    (\"minutes_played\", \"avg_minutes\"),\n",
    "    (\"points\", \"avg_points\"),\n",
    "    (\"free_throws\", \"avg_free_throws\"),\n",
    "    (\"blocks\", \"avg_blocks\")\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[new_col] = df[col].apply(\n",
    "            lambda x: round(sum(x)/len(x), 1) if isinstance(x, list) and len(x) > 0\n",
    "            else round(float(x), 1) if x is not None else 0\n",
    "        )\n",
    "    else:\n",
    "        df[new_col] = 0\n",
    "\n",
    "# Drop the original list or raw columns (except total_games)\n",
    "for col in [\"minutes_played\", \"points\", \"free_throws\", \"blocks\"]:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# --- Ensure numeric columns are proper types ---\n",
    "numeric_cols = [\"weight\", \"avg_minutes\", \"avg_points\", \"avg_free_throws\", \"avg_blocks\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# Convert total_games to numeric but do not modify it otherwise\n",
    "if \"total_games\" in df.columns:\n",
    "    df[\"total_games\"] = pd.to_numeric(df[\"total_games\"], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- Clean string columns ---\n",
    "string_cols = [\"name\", \"position\", \"url\"]\n",
    "for col in string_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# --- Optional: reorder columns ---\n",
    "cols_order = [\n",
    "    \"name\", \"position\", \"weight\", \"avg_minutes\", \"avg_points\",\n",
    "    \"avg_free_throws\", \"avg_blocks\", \"total_games\", \"url\"\n",
    "]\n",
    "df = df[[c for c in cols_order if c in df.columns]]\n",
    "\n",
    "# --- Display the cleaned DataFrame ---\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
